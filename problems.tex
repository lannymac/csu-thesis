\section{Problems with calculation of the Multifractal Spectrum}

As alluded to repeatedly, the multifractal spectrum is difficult to compute, requiring large amounts of data.  This slow convergence is especially pronounced for the negative $q$ portion of the spectrum.  This is because the negative $q$ part of the spectrum is controlled by the sparsest parts of the attractor, which by definition are the parts that are populated the slowest for most processes.

One interpretation of the multifractal spectrum $D_q$ is that the value of $q$ picks out which areas of the attractor to focus on.  This is an oversimplification, but it is phenominologically correct.  Recall that $D_q$ is found by examining $\Gamma=\sum\frac{p^q}{\ell^\tau}$.  In the $q\to+\infty$ limit, the terms that dominate the sum are the terms with the largest values of $p$; the densest parts of the attractor.  In that limit, $D_\infty$ can be loosely thought of as the dimension of the dense parts of the attractor.  Similarly, as $q\to-\infty$ limit, the terms that dominate the sum are the smallest values of $p$, which allows the interpretation of $D_{-\infty}$ as the dimension of the support of the sparsest areas of the attractor.  The more correct interpretation would be to examine $f(\alpha)$, and make arguments about the extremal values of $\alpha$, since $f$ really is defined as the dimension of the support of certain sets of points.  However, this looser interpretation makes a better link with the actual data.

Clearly, if the attractor is populated by a process that randomly places points selected according to the natural measure of the attractor %check phrasing
then these sparse areas will be rarely visited, making it hard to get an accurate estimate of the measure of the attractor contained in a box which only encompasses sparse parts of the attractor.  Further complicating matters is the fact that for any real measurement of a system, the populating process is not continuous.  Discrete measurements are made, so that sparsely occupied cells in a histogram will usually have either zero, one, or two points in them.  This means that when computing the partition function $\Gamma$, instead of having a collection of terms that have small $p$ values that are not fully converged, but still change smoothly, there will be a collection of cells that have only one point in them.  If the number of singly occupied cells is denoted by $N_{\{n=1\}}=N_1$, then once $N_1 \approx N_2 2^q \Rightarrow q_c \approx \log (N_1/N_2) / \log 2$, the $n=1$ terms will dominate the sum.  

Notice that if we are computing $\tau$ for $q$ more negative than $q_c$, we will effectively have $\sum n^q \approx N_1 1^q = N_1$.  This quantity will depend on $\ell$, but it is completely independent of $q$.  When finding our slope of the $\sum p^q$ vs. $\log\ell$ curves, we will get $\tau \approx \tau_0$, giving $D_q = \tau/(q-1) = \tau_0/(q-1)$.  If if so happens that $N_1$ varies in just the right manner so that $\tau_0 = D_{-\infty}$, then this will produce the correct $D_q$ spectrum, but of course the sparse areas of the attractor were the areas controlling $N_1$, and it is exceedingly rare for these areas to have converged well enough to produce the correct value of $D_{-\infty}$ at a stage where many cells are still only singly occupied.  The more general behavior is that there will be a small number of mostly isolated singly occupied cells, so that as the histogram is re-binned, $N_1$ will stay mostly constant, then make abrupt drops as singly occupied cells get binned into the same cell as each other at some particular length scale.  This means that often $\tau_0$ is not negative enough, since $N_1$ is staying more constant than it should.  This leads to an underestimate of $D_{-\infty}$, which can produce $D_q$ curves that have $D_{-\infty}$ lower than $D_\infty$, which violates the requirement that all $D_q$ curves decrease monotonically with $q$. %ref?

% If we are computing $D_q$ for a $q$ value past the $q_c$ threshold, then the partition function becomes basically $\Gamma = \sum \frac{p^q}{\ell^\tau} = \frac{N_1(1/N)^q}{\ell^\tau}$.  Note that the factor of $1/N$ that scales between probability and number of cells does not depend on the box size.  This means that when solving $\Gamma = A$ using a log-log plot, that factor will provide a $q$ dependent offset.  Since $\tau$ is the slope of the $\log\sum p^q$ vs. $\log\ell$ curve, this offset will not affect the calculation of $\tau$, and thus will not change the resulting $D_q$.  There will be a different value of the offset for each different value of $q$ selected, but the fitting is performed at fixed $q$.  Effectively, we can solve $\Gamma = B(q, N)$ instead of $\Gamma = A(N)$.  The result is the same.  %Our algorithm described in section \ref{multi} will find a slope, but we can simply examine this new approximate equation and find an actual solution. We have
% % Recall that our algorithm from section \ref{multi} was finding the slope of 
% 
% This section is not finished.  There is a description in the research notes of how exactly $D_q$ becomes independent of $q$, but I have a sneaking suspicion that that result was from before our least-squares fitting, back when we just took a ratio.  It may need to be re-examined.  The basic result, though, is that past the $q_c$ boundary, we have $D_q \approx a\frac{q}{q-1}$, or something similar.
% 
% 
% 
% Consider the sum performed for $\gamma(q,\ell) = \sum p_i^q$.  Of course, we actually sum up $\sum n_i^q$, where $n_i$ is the number of points in the $i^{th}$ cell, but this is completely equivalent, since the calculation of $\tau$ is done by finding the slope of $\log\gamma$ vs $\log\ell$.  The overall factor of $N$ needed to convert between $n$ and $p$ is just a $q$ dependent offset to the graphs.  Now, examine the limit as $q\to -\infty$.  The sum will be dominated by any cells that have $n_i =1$.  Any cells with $n_i > 1$ will make a vanishing contribution to the sum.  Thus, for highly negative $q$, we expect $\gamma(\ell) \approx N_{\{n=1\}}(\ell)$.  The explicit $\ell$ dependence is hard to pin down, but $N_{\{n=1\}}$ does depend on $\ell$, so that we will get some value for the slope of our $\gamma\ell$ plots.  However, notice that this quantity becomes independent of $q$.  The $q$ independence is also hard to explicitly quantify, but empirically, it tends to take over quite quickly, near $q_c\approx -2$.  To the left of there, $\gamma$ (and thus $\tau$) stays more or less fixed at a constant value, but recall that $D_q = \tau/(q-1)$, so that as $q \to -\infty$, $D_q$ now falls off more or less as $1/q$.  This leads to the $D_q$ curves asymptotically approaching some constant value, which is not the correct $D_{-\infty}$.% smooth ``backwards" behavior.  
% 
% Notice that removing the $n=1$ cells does not fix the problem; the $n=2$ cells then play a similar role.  The real problem here is that we have $p$ not smoothly varying in any moderately realistic experiment.  The difference between the least occupied cells and the next least occupied cells is too drastic, and the populations of these two types of cell are too large.  Basically, we need more points per cell, although requiring a higher average occupancy is not quite enough.  We need to have even the sparsest parts of the attractor well-populated enough that there is smooth variation between cells.  This means that very large amounts of data are required, since the sparse areas of the attractor are visited with the least frequency.

% Of course, we started looking for solutions before making sure that we had accurately diagnosed the problem, but I eventually went back to the 1D case and tried using way too little data.  The results are in figure \ref{2010_09_sparse1d}.  The 1D results supported out theory, and suggested that perhaps if I used 1000 points per cell, the curves might switch over to the expected behavior.  This turned out not to be the case.

A phenominological demonstration of this behavior is shown in figure \ref{2010_09_sparse1d}.  Various $D_q$ curves were computed for the logistic map, with different numbers of points used to populate the histogram.  When the histogram is too sparse, the curves asymptotically approach a wide spread of values for $q\to -\infty$.  These values are not related to the actual $D_{-\infty}$ result.

\begin{figure}[!ht]
\includegraphics[width=.9\textwidth]{20100922_progression.png}
\caption[Plots demonstrating the effect of sparse data on $D_q$ convergence.]{Plots of the $D_q$ curve for the logistic map as the total number of points is varied.  There were 10,000 bins in the histogram.  Notice that $D_q$ settles down to the expected curve in the general vacinity of 100 points per cell, although in reality this number is slightly higher, since some cells are empty.  Also notice that although $N_{\{n=1\}}=0$ appears to be a necessary requirement, it is not sufficient.  All these curves were calculated with the naive algorithm. \label{2010_09_sparse1d}}
\end{figure}

This type of error is known as a clipping error, %ref?
and there are several existing methods that attempt to provide better estimates of $D_q$ for negative $q$.  We are most concerned with the Extended Box Algorithm (EBA).  